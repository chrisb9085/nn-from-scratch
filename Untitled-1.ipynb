{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a8f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itereation 1's Loss: 36.0\n",
      "Itereation 2's Loss: 33.872399999999985\n",
      "Itereation 3's Loss: 31.870541159999995\n",
      "Itereation 4's Loss: 29.98699217744401\n",
      "Itereation 5's Loss: 28.21476093975706\n",
      "Itereation 6's Loss: 26.54726856821742\n",
      "Itereation 7's Loss: 24.978324995835766\n",
      "Itereation 8's Loss: 23.502105988581878\n",
      "Itereation 9's Loss: 22.113131524656684\n",
      "Itereation 10's Loss: 20.80624545154949\n",
      "Itereation 11's Loss: 19.576596345362915\n",
      "Itereation 12's Loss: 18.419619501351963\n",
      "Itereation 13's Loss: 17.331019988822064\n",
      "Itereation 14's Loss: 16.306756707482677\n",
      "Itereation 15's Loss: 15.343027386070442\n",
      "Itereation 16's Loss: 14.43625446755368\n",
      "Itereation 17's Loss: 13.583071828521266\n",
      "Itereation 18's Loss: 12.780312283455652\n",
      "Itereation 19's Loss: 12.024995827503426\n",
      "Itereation 20's Loss: 11.314318574097976\n",
      "Itereation 21's Loss: 10.645642346368787\n",
      "Itereation 22's Loss: 10.016484883698395\n",
      "Itereation 23's Loss: 9.424510627071816\n",
      "Itereation 24's Loss: 8.867522049011871\n",
      "Itereation 25's Loss: 8.34345149591527\n",
      "Itereation 26's Loss: 7.850353512506679\n",
      "Itereation 27's Loss: 7.386397619917536\n",
      "Itereation 28's Loss: 6.949861520580408\n",
      "Itereation 29's Loss: 6.539124704714106\n",
      "Itereation 30's Loss: 6.152662434665503\n",
      "Itereation 31's Loss: 5.7890400847767705\n",
      "Itereation 32's Loss: 5.446907815766464\n",
      "Itereation 33's Loss: 5.124995563854671\n",
      "Itereation 34's Loss: 4.8221083260308575\n",
      "Itereation 35's Loss: 4.537121723962434\n",
      "Itereation 36's Loss: 4.268977830076255\n",
      "Itereation 37's Loss: 4.016681240318748\n",
      "Itereation 38's Loss: 3.7792953790159096\n",
      "Itereation 39's Loss: 3.5559390221160707\n",
      "Itereation 40's Loss: 3.345783025909011\n",
      "Itereation 41's Loss: 3.148047249077789\n",
      "Itereation 42's Loss: 2.9619976566572896\n",
      "Itereation 43's Loss: 2.786943595148845\n",
      "Itereation 44's Loss: 2.622235228675549\n",
      "Itereation 45's Loss: 2.4672611266608238\n",
      "Itereation 46's Loss: 2.321445994075166\n",
      "Itereation 47's Loss: 2.1842485358253256\n",
      "Itereation 48's Loss: 2.0551594473580463\n",
      "Itereation 49's Loss: 1.9336995240191863\n",
      "Itereation 50's Loss: 1.8194178821496518\n",
      "Itereation 51's Loss: 1.7118902853146067\n",
      "Itereation 52's Loss: 1.6107175694525138\n",
      "Itereation 53's Loss: 1.515524161097869\n",
      "Itereation 54's Loss: 1.4259566831769857\n",
      "Itereation 55's Loss: 1.3416826432012259\n",
      "Itereation 56's Loss: 1.2623891989880334\n",
      "Itereation 57's Loss: 1.18778199732784\n",
      "Itereation 58's Loss: 1.1175840812857634\n",
      "Itereation 59's Loss: 1.0515348620817766\n",
      "Itereation 60's Loss: 0.9893891517327431\n",
      "Itereation 61's Loss: 0.930916252865338\n",
      "Itereation 62's Loss: 0.8758991023209968\n",
      "Itereation 63's Loss: 0.8241334653738256\n",
      "Itereation 64's Loss: 0.7754271775702323\n",
      "Itereation 65's Loss: 0.7295994313758316\n",
      "Itereation 66's Loss: 0.6864801049815187\n",
      "Itereation 67's Loss: 0.6459091307771115\n",
      "Itereation 68's Loss: 0.6077359011481847\n",
      "Itereation 69's Loss: 0.571818709390327\n",
      "Itereation 70's Loss: 0.5380242236653578\n",
      "Itereation 71's Loss: 0.5062269920467349\n",
      "Itereation 72's Loss: 0.47630897681677353\n",
      "Itereation 73's Loss: 0.4481591162869011\n",
      "Itereation 74's Loss: 0.4216729125143454\n",
      "Itereation 75's Loss: 0.3967520433847474\n",
      "Itereation 76's Loss: 0.3733039976207088\n",
      "Itereation 77's Loss: 0.35124173136132436\n",
      "Itereation 78's Loss: 0.3304833450378702\n",
      "Itereation 79's Loss: 0.3109517793461322\n",
      "Itereation 80's Loss: 0.29257452918677535\n",
      "Itereation 81's Loss: 0.27528337451183676\n",
      "Itereation 82's Loss: 0.25901412707818716\n",
      "Itereation 83's Loss: 0.24370639216786655\n",
      "Itereation 84's Loss: 0.22930334439074554\n",
      "Itereation 85's Loss: 0.21575151673725296\n",
      "Itereation 86's Loss: 0.2030006020980815\n",
      "Itereation 87's Loss: 0.1910032665140846\n",
      "Itereation 88's Loss: 0.17971497346310225\n",
      "Itereation 89's Loss: 0.16909381853143338\n",
      "Itereation 90's Loss: 0.1591003738562249\n",
      "Itereation 91's Loss: 0.14969754176132236\n",
      "Itereation 92's Loss: 0.14085041704322837\n",
      "Itereation 93's Loss: 0.13252615739597357\n",
      "Itereation 94's Loss: 0.12469386149387143\n",
      "Itereation 95's Loss: 0.11732445427958357\n",
      "Itereation 96's Loss: 0.1103905790316602\n",
      "Itereation 97's Loss: 0.1038664958108892\n",
      "Itereation 98's Loss: 0.09772798590846558\n",
      "Itereation 99's Loss: 0.09195226194127534\n",
      "Itereation 100's Loss: 0.08651788326054576\n",
      "Itereation 101's Loss: 0.08140467635984766\n",
      "Itereation 102's Loss: 0.07659365998698062\n",
      "Itereation 103's Loss: 0.07206697468175022\n",
      "Itereation 104's Loss: 0.06780781647805834\n",
      "Itereation 105's Loss: 0.06380037452420508\n",
      "Itereation 106's Loss: 0.06002977238982451\n",
      "Itereation 107's Loss: 0.056482012841585764\n",
      "Itereation 108's Loss: 0.05314392588264784\n",
      "Itereation 109's Loss: 0.050003119862983315\n",
      "Itereation 110's Loss: 0.04704793547908108\n",
      "Itereation 111's Loss: 0.044267402492267266\n",
      "Itereation 112's Loss: 0.04165119900497404\n",
      "Itereation 113's Loss: 0.03918961314378035\n",
      "Itereation 114's Loss: 0.036873507006982977\n",
      "Itereation 115's Loss: 0.034694282742870286\n",
      "Itereation 116's Loss: 0.032643850632766785\n",
      "Itereation 117's Loss: 0.030714599060370322\n",
      "Itereation 118's Loss: 0.028899366255902493\n",
      "Itereation 119's Loss: 0.027191413710178584\n",
      "Itereation 120's Loss: 0.025584401159906914\n",
      "Itereation 121's Loss: 0.024072363051356495\n",
      "Itereation 122's Loss: 0.02264968639502138\n",
      "Itereation 123's Loss: 0.021311089929075627\n",
      "Itereation 124's Loss: 0.02005160451426725\n",
      "Itereation 125's Loss: 0.018866554687474075\n",
      "Itereation 126's Loss: 0.017751541305444478\n",
      "Itereation 127's Loss: 0.016702425214292563\n",
      "Itereation 128's Loss: 0.015715311884128023\n",
      "Itereation 129's Loss: 0.014786536951776086\n",
      "Itereation 130's Loss: 0.013912652617925996\n",
      "Itereation 131's Loss: 0.013090414848206543\n",
      "Itereation 132's Loss: 0.012316771330677616\n",
      "Itereation 133's Loss: 0.011588850145034585\n",
      "Itereation 134's Loss: 0.010903949101463065\n",
      "Itereation 135's Loss: 0.010259525709566468\n",
      "Itereation 136's Loss: 0.00965318774013127\n",
      "Itereation 137's Loss: 0.009082684344689475\n",
      "Itereation 138's Loss: 0.008545897699918217\n",
      "Itereation 139's Loss: 0.008040835145853157\n",
      "Itereation 140's Loss: 0.007565621788733219\n",
      "Itereation 141's Loss: 0.0071184935410191314\n",
      "Itereation 142's Loss: 0.0066977905727448606\n",
      "Itereation 143's Loss: 0.0063019511498957235\n",
      "Itereation 144's Loss: 0.0059295058369368625\n",
      "Itereation 145's Loss: 0.005579072041973911\n",
      "Itereation 146's Loss: 0.005249348884293189\n",
      "Itereation 147's Loss: 0.004939112365231465\n",
      "Itereation 148's Loss: 0.004647210824446307\n",
      "Itereation 149's Loss: 0.004372560664721486\n",
      "Itereation 150's Loss: 0.004114142329436494\n",
      "Itereation 151's Loss: 0.003870996517766834\n",
      "Itereation 152's Loss: 0.0036422206235667827\n",
      "Itereation 153's Loss: 0.003426965384714017\n",
      "Itereation 154's Loss: 0.0032244317304774505\n",
      "Itereation 155's Loss: 0.0030338678152062068\n",
      "Itereation 156's Loss: 0.0028545662273275238\n",
      "Itereation 157's Loss: 0.002685861363292443\n",
      "Itereation 158's Loss: 0.002527126956721865\n",
      "Itereation 159's Loss: 0.0023777737535795864\n",
      "Itereation 160's Loss: 0.00223724732474303\n",
      "Itereation 161's Loss: 0.0021050260078507234\n",
      "Itereation 162's Loss: 0.0019806189707867374\n",
      "Itereation 163's Loss: 0.0018635643896132343\n",
      "Itereation 164's Loss: 0.0017534277341871227\n",
      "Itereation 165's Loss: 0.001649800155096641\n",
      "Itereation 166's Loss: 0.0015522969659304577\n",
      "Itereation 167's Loss: 0.001460556215243966\n",
      "Itereation 168's Loss: 0.0013742373429230384\n",
      "Itereation 169's Loss: 0.0012930199159562866\n",
      "Itereation 170's Loss: 0.0012166024389232565\n",
      "Itereation 171's Loss: 0.0011447012347829176\n",
      "Itereation 172's Loss: 0.0010770493918072417\n",
      "Itereation 173's Loss: 0.0010133957727514104\n",
      "Itereation 174's Loss: 0.0009535040825818078\n",
      "Itereation 175's Loss: 0.0008971519913012032\n",
      "Itereation 176's Loss: 0.0008441303086153036\n",
      "Itereation 177's Loss: 0.0007942422073761319\n",
      "Itereation 178's Loss: 0.0007473024929201971\n",
      "Itereation 179's Loss: 0.0007031369155886336\n",
      "Itereation 180's Loss: 0.0006615815238773228\n",
      "Itereation 181's Loss: 0.0006224820558161947\n",
      "Itereation 182's Loss: 0.0005856933663174669\n",
      "Itereation 183's Loss: 0.0005510788883681015\n",
      "Itereation 184's Loss: 0.0005185101260655451\n",
      "Itereation 185's Loss: 0.00048786617761505856\n",
      "Itereation 186's Loss: 0.00045903328651801555\n",
      "Itereation 187's Loss: 0.0004319044192847635\n",
      "Itereation 188's Loss: 0.0004063788681050637\n",
      "Itereation 189's Loss: 0.0003823618770000461\n",
      "Itereation 190's Loss: 0.0003597642900693548\n",
      "Itereation 191's Loss: 0.0003385022205262612\n",
      "Itereation 192's Loss: 0.00031849673929316324\n",
      "Itereation 193's Loss: 0.0002996735820009465\n",
      "Itereation 194's Loss: 0.0002819628733046985\n",
      "Itereation 195's Loss: 0.0002652988674923804\n",
      "Itereation 196's Loss: 0.0002496197044235683\n",
      "Itereation 197's Loss: 0.00023486717989212869\n",
      "Itereation 198's Loss: 0.00022098652956051694\n",
      "Itereation 199's Loss: 0.0002079262256634926\n",
      "Itereation 200's Loss: 0.00019563778572677352\n",
      "Final weights: [-3.3990955  -0.20180899  0.80271349]\n",
      "Final bias: 0.6009044964039992\n",
      "Final confidence: 0.9998043622142733\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial paramaters\n",
    "weights = np.array([-3.0, -1.0, 2.0])\n",
    "bias = 1.0\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "target_output = 0.0\n",
    "learning_rate = 0.001 # Or step size\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    if x > 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "for i in range(200):\n",
    "    # Forward Pass\n",
    "    linear_output = np.dot(weights, inputs) + bias\n",
    "    relu_output = relu(linear_output)\n",
    "    loss = (relu_output - target_output) **2\n",
    "\n",
    "    # Backward Pass\n",
    "    dloss_doutput = 2 * (relu_output - target_output) # Derivative of Loss\n",
    "    doutput_dlinear = relu_derivative(linear_output) # Derivative of ReLU w/r to dot prod.\n",
    "    dlinear_dweights = inputs # Derivative of \"mul\", dot product w/r to w is the input\n",
    "    dlinear_dbias = 1.0 # Derivative of bias is 1\n",
    "\n",
    "    dloss_dlinear = dloss_doutput * doutput_dlinear # Deriv of loss w respect to linear inputs, x\n",
    "    dloss_dweights = dloss_dlinear * dlinear_dweights # Deriv of loss w respect to weights\n",
    "    dloss_dbias = dloss_dlinear * dlinear_dbias # Deriv of loss w respect to bias\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    # Print the loss for this iteration\n",
    "    print(f\"Itereation {i + 1}'s Loss: {loss}\")\n",
    "\n",
    "print(\"Final weights:\", weights)\n",
    "print(\"Final bias:\", bias)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
